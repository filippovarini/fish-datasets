{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Template\n",
    "This notebook was used to download, analyze and process the data from the NOAA Pudget Sound dataset.\n",
    "\n",
    "You can use this template to process your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import supervision as sv\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Data\n",
    "- The data is located at: \"https://bit.ly/fishclef-2015\"\n",
    "- Manually download the zip file. The zip is named *fishclef_2015_release.zip*\n",
    "- Assign the path of the downloaded zip file to the *data_path* variable\n",
    "- Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/tmp/data/fishclef_2015_release.zip\")\n",
    "dataset_shortname = \"fishclef\"\n",
    "data_dir = Path(\"/tmp/data/\") / dataset_shortname\n",
    "data_dir.mkdir(exist_ok=True, parents=True)\n",
    "!unzip {data_path} -d {data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_annotations_path = data_dir / Path(\"fishclef_2015_release/training_set/gt\")\n",
    "training_annotations_coco_path = data_dir / Path(\"fishclef_2015_release/training_set/gt_coco\")\n",
    "training_videos_path = data_dir / Path(\"fishclef_2015_release/training_set/videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotations_path = data_dir / Path(\"fishclef_2015_release/test_set/gt\")\n",
    "test_annotations_coco_path = data_dir / Path(\"fishclef_2015_release/test_set/gt_coco\")\n",
    "test_videos_path = data_dir / Path(\"fishclef_2015_release/test_set/videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the annotations\n",
    "Turn into COCO format readable by `supervision` library, for easy visualization and conversion to other formats.\n",
    "- one \"video\".json is created per video\n",
    "- \"video\".json only contains annotations for images with at least one bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xml_to_coco(xml_file, output_dir=None):\n",
    "    \"\"\"\n",
    "    Converts a single XML annotation file to COCO JSON format.\n",
    "    \n",
    "    Parameters:\n",
    "        xml_file (str): Path to the XML file.\n",
    "        output_dir (str): Directory where the JSON file will be saved. \n",
    "                          If None, the JSON is saved in the same directory as xml_file.\n",
    "        width (int): Default image width.\n",
    "        height (int): Default image height.\n",
    "    \n",
    "    Returns:\n",
    "        output_json (str): Path to the generated COCO JSON file.\n",
    "    \"\"\"\n",
    "    # Parse XML\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract the video id/name from the <video> element.\n",
    "    video_id = root.get(\"id\")\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = {}\n",
    "    ann_id = 1\n",
    "\n",
    "    # Process each <frame> element\n",
    "    for frame in root.findall(\"frame\"):\n",
    "        frame_id = frame.get(\"id\")\n",
    "        # Create image entry; using the video id and frame id in the file name.\n",
    "        image_info = {\n",
    "            \"id\": int(frame_id),\n",
    "            \"file_name\": f\"{video_id}_frame_{frame_id}.jpg\",\n",
    "        }\n",
    "        images.append(image_info)\n",
    "\n",
    "        # Process each <object> in the frame\n",
    "        for obj in frame.findall(\"object\"):\n",
    "            species = obj.get(\"fish_species\")\n",
    "            if species not in categories:\n",
    "                categories[species] = len(categories) + 1  # assign new id\n",
    "\n",
    "            # Extract bounding box coordinates: top-left x, top-left y, width, height\n",
    "            x = int(obj.get(\"x\"))\n",
    "            y = int(obj.get(\"y\"))\n",
    "            w = int(obj.get(\"w\"))\n",
    "            h = int(obj.get(\"h\"))\n",
    "\n",
    "            ann = {\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": int(frame_id),\n",
    "                \"category_id\": categories[species],\n",
    "                \"bbox\": [x, y, w, h],\n",
    "            }\n",
    "            annotations.append(ann)\n",
    "            ann_id += 1\n",
    "\n",
    "    # Create categories list for COCO format\n",
    "    categories_list = [\n",
    "        {\"id\": cat_id, \"name\": species}\n",
    "        for species, cat_id in categories.items()\n",
    "    ]\n",
    "\n",
    "    # Assemble the final COCO dictionary\n",
    "    coco_dict = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories_list\n",
    "    }\n",
    "\n",
    "    # Determine output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(xml_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    base_name = os.path.splitext(os.path.basename(xml_file))[0]\n",
    "    output_json = os.path.join(output_dir, base_name + \".json\")\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(coco_dict, f, indent=4)\n",
    "\n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labels for supervision library\n",
    "- Convert xml files to json files for both training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training annotations\n",
    "xml_files = glob.glob(os.path.join(training_annotations_path, \"*.xml\"))\n",
    "for xml_file in xml_files:\n",
    "    output_json = convert_xml_to_coco(xml_file, training_annotations_coco_path)\n",
    "    print(f\"Converted '{xml_file}' to '{output_json}'.\")\n",
    "\n",
    "# Convert test annotations\n",
    "xml_files = glob.glob(os.path.join(test_annotations_path, \"*.xml\"))\n",
    "for xml_file in xml_files:\n",
    "    output_json = convert_xml_to_coco(xml_file, test_annotations_coco_path)\n",
    "    print(f\"Converted '{xml_file}' to '{output_json}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup infrastructure to visualise a few frames from a video\n",
    "- To visualise we need to extract the frames from the video\n",
    "- Only extract the frames(16 annotated frames) that we intend to visualise\n",
    "- For the extracted images, create supervision detection objects for the annotations\n",
    "- Display the extracted frames with the annotations using matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frame(cap, frame_index):\n",
    "    # Set video position to the desired frame index and read it\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(f\"Frame {frame_index} could not be read\")\n",
    "    return frame\n",
    "\n",
    "def detections_from_coco(coco_data, image_id):\n",
    "    \"\"\"\n",
    "    Converts COCO annotations for a given image_id into a supervision.Detections object.\n",
    "    Assumes bounding boxes in COCO are in [x, y, w, h] format.\n",
    "    \"\"\"\n",
    "    anns = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] == image_id]\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    \n",
    "    for ann in anns:\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        # Convert from [x, y, w, h] to [x1, y1, x2, y2]\n",
    "        boxes.append([x, y, x + w, y + h])\n",
    "        confidences.append(1.0)  # No score provided; assume full confidence.\n",
    "        class_ids.append(ann[\"category_id\"])\n",
    "    \n",
    "    if boxes:\n",
    "        boxes = np.array(boxes)\n",
    "        confidences = np.array(confidences)\n",
    "        class_ids = np.array(class_ids)\n",
    "    else:\n",
    "        boxes = np.empty((0, 4))\n",
    "        confidences = np.empty((0,))\n",
    "        class_ids = np.empty((0,))\n",
    "    \n",
    "    return sv.Detections(xyxy=boxes, confidence=confidences, class_id=class_ids)\n",
    "\n",
    "def visualize_annotated_frames_grid(video_path, coco_json_path, num_images=16, grid_shape=(4,4)):\n",
    "    # Load COCO annotations\n",
    "    with open(coco_json_path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Build a set of image_ids that have at least one annotation\n",
    "    annotated_frame_ids = {ann[\"image_id\"] for ann in coco_data[\"annotations\"]}\n",
    "    \n",
    "    # Filter the images list to only those with annotations, sorted by image_id\n",
    "    annotated_images = sorted([img for img in coco_data[\"images\"] if img[\"id\"] in annotated_frame_ids],\n",
    "                              key=lambda x: x[\"id\"])\n",
    "    \n",
    "    if len(annotated_images) == 0:\n",
    "        print(\"No annotated frames found.\")\n",
    "        return\n",
    "    \n",
    "    # Choose num_images frames at equal intervals from annotated_images\n",
    "    if len(annotated_images) < num_images:\n",
    "        selected_images = annotated_images\n",
    "    else:\n",
    "        indices = np.linspace(0, len(annotated_images) - 1, num_images, dtype=int)\n",
    "        selected_images = [annotated_images[i] for i in indices]\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(f\"Error opening video file: {video_path}\")\n",
    "    \n",
    "    # Initialize BoxAnnotator from supervision\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    \n",
    "    annotated_frames = []\n",
    "    selected_ids = []  # to keep track of the selected frame ids\n",
    "    \n",
    "    for image_info in selected_images:\n",
    "        frame_id = image_info[\"id\"]\n",
    "        # Extract the frame using frame_id (assuming image_id matches frame index)\n",
    "        frame = extract_frame(cap, frame_id)\n",
    "        \n",
    "        # Get detections for this frame\n",
    "        detections = detections_from_coco(coco_data, image_id=frame_id)\n",
    "        \n",
    "        # Annotate the frame\n",
    "        annotated = box_annotator.annotate(scene=frame, detections=detections)\n",
    "        \n",
    "        # Convert from BGR (OpenCV) to RGB for matplotlib display\n",
    "        annotated = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "        annotated_frames.append(annotated)\n",
    "        selected_ids.append(frame_id)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Plot the selected frames in a grid\n",
    "    fig, axes = plt.subplots(grid_shape[0], grid_shape[1], figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img in enumerate(annotated_frames):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(f\"Frame {selected_ids[i]}\")\n",
    "    \n",
    "    # Hide any remaining subplots if necessary\n",
    "    for j in range(len(annotated_frames), len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{dataset_shortname}_sample_image.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise\n",
    "- Randomly choose a video from the training set and the test set and visualize 16 annotated frames from each picked video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise a video from training set\n",
    "video_files = glob.glob(os.path.join(training_videos_path, \"*.flv\"))\n",
    "video_file = random.choice(video_files)\n",
    "coco_annotations = training_annotations_coco_path / (os.path.splitext(os.path.basename(video_file))[0] + \".json\")\n",
    "print(f\"Video file: {video_file}\")\n",
    "visualize_annotated_frames_grid(video_file, coco_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise a video from test set\n",
    "video_files = glob.glob(os.path.join(test_videos_path, \"*.flv\"))\n",
    "video_file = random.choice(video_files)\n",
    "coco_annotations = test_annotations_coco_path / (os.path.splitext(os.path.basename(video_file))[0] + \".json\")\n",
    "print(f\"Video file: {video_file}\")\n",
    "visualize_annotated_frames_grid(video_file, coco_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Output\n",
    "- Save example image\n",
    "- Save notebook to visualize the image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish_detection_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
